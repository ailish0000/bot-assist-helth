"""
üßπ –ú–æ–¥—É–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ—Ç–∞-–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∞
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–æ—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import re
import logging
from typing import List, Dict, Tuple, Set
from collections import Counter
import unicodedata

logger = logging.getLogger(__name__)

class DataCleaner:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–∏—Å—Ç–∏—Ç–µ–ª—è –¥–∞–Ω–Ω—ã—Ö"""
        self._init_cleaning_rules()
        logger.info("‚úÖ –ú–æ–¥—É–ª—å –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _init_cleaning_rules(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –æ—á–∏—Å—Ç–∫–∏"""
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π/–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ
        self.typo_corrections = {
            # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            "—Ç–µ–π–ø–∏—Ä–æ–≤–∞–Ω–∏–µ": ["—Ç–µ–π–ø–∏—Ä–æ–≤–∞–Ω–µ–µ", "—Ç–µ–π–ø–∏—Ä–≤–∞–Ω—å–µ", "—Ç–µ–π–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–µ", "—Ç—ç–π–ø–∏—Ä–æ–≤–∞–Ω–∏–µ"],
            "–∫–∏–Ω–µ–∑–∏–æ—Ç–µ–π–ø–∏–Ω–≥": ["–∫–∏–Ω–µ–∑–∏–æ—Ç–µ–π–ø–∏–Ω–∫", "–∫–∏–Ω–µ–∑–∏–æ—Ç—ç–π–ø–∏–Ω–≥", "–∫–∏–Ω–µ–∑–∏–æ—Ç–µ–ø–∏–Ω–≥"],
            "–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏—è": ["–Ω—É—Ç—Ä–∏—Ü–∏–∞–ª–æ–≥–∏—è", "–Ω—É—Ç—Ä–∏—Ü–µ–æ–ª–æ–≥–∏—è", "–Ω—É—Ç—Ä–µ—Ü–∏–æ–ª–æ–≥–∏—è"],
            "–≤–∏—Ç–∞–º–∏–Ω—ã": ["–≤–∏—Ç–∞–º–∏–Ω–Ω—ã", "–≤–∏—Ç–∞–º—ã–Ω—ã", "–≤—ã—Ç–∞–º–∏–Ω—ã"],
            "–±–µ–ª–∫–∏": ["–±–µ–ª—å–∫–∏", "–±–µ–ª–∫–∫–∏"],
            "—É–≥–ª–µ–≤–æ–¥—ã": ["—É–≥–ª–µ–≤–æ–¥–¥—ã", "—É–≥–ª–µ–≤–æ–Ω—ã", "—É–≥–ª–∏–≤–æ–¥—ã"],
            "–∫–∞–ª–æ—Ä–∏–∏": ["–∫–∞–ª–ª–æ—Ä–∏–∏", "–∫–æ–ª–æ—Ä–∏–∏", "–∫–∞–ª–ª–æ—Ä–∏–π"],
            "–¥–∏–µ—Ç–∞": ["–¥–∏—ç—Ç–∞", "–¥–∏–µ—Ç—Ç–∞"],
            
            # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            "–∫—É—Ä–∞—Ç–æ—Ä": ["–∫—É—Ä–æ—Ç–æ—Ä", "–∫—É—Ä–∞—Ç—Ç–æ—Ä", "–∫—É—Ä–∞—Ç–∞—Ä"],
            "—ç–∫–∑–∞–º–µ–Ω": ["—ç–∫–∑–∞–º–º–µ–Ω", "—ç–∫–∑–∞–º–µ–µ–Ω", "—ç–∫–∑–∞–º–Ω–µ"],
            "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç": ["—Å–µ—Ä—Ç–µ—Ñ–∏–∫–∞—Ç", "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—Ç", "—Å–µ—Ä—Ç–∏—Ñ–µ–∫–∞—Ç"],
            "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ": ["—Ä–æ—Å–ø–∏—Å–∞–Ω–∏–µ", "—Ä–∞—Å–ø–∏—Å–∞–Ω–µ–µ", "—Ä–∞—Å–ø–µ—Å–∞–Ω–∏–µ"],
            
            # –ö—É–ª–∏–Ω–∞—Ä–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            "—Ä–µ—Ü–µ–ø—Ç": ["—Ä–µ—Ü–µ–ø—Ç", "—Ä–µ—Ü–µ–ø–ø—Ç", "—Ä–µ—Ü–µ–ø—Ä"],
            "–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã": ["–∏–Ω–≥—Ä–∏–¥–∏–µ–Ω—Ç—ã", "–∏–Ω–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã", "–∏–Ω–≥—Ä–µ–¥–µ–µ–Ω—Ç—ã"],
            "–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ": ["–ø—Ä–µ–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ", "–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–µ–µ", "–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ–µ"],
            "–º–∞–π–æ–Ω–µ–∑": ["–º–∞–π–æ–Ω—ç–∑", "–º–∞–π–æ–Ω–µ–π–∑", "–º–∞–µ–Ω–µ–∑"],
            
            # –û–±—â–∏–µ —Å–ª–æ–≤–∞
            "–ø–æ–º–æ–≥–∞–µ—Ç": ["–ø–æ–º–∞–≥–∞–µ—Ç", "–ø–æ–º–æ–≥–∞–µ—Ç—å", "–ø–æ–º–∞–≥–∞–µ—Ç—å"],
            "–ø–æ–ª–µ–∑–Ω–æ": ["–ø–æ–ª–µ–∑–∑–Ω–æ", "–ø–æ–ª–µ–∑–Ω–Ω–æ", "–ø–æ–ª–µ–∑–µ–Ω"],
            "–º–æ–∂–Ω–æ": ["–º–æ–∂–∂–Ω–æ", "–º–æ—à–Ω–æ", "–º–æ–∂–µ–æ"],
            "–Ω—É–∂–Ω–æ": ["–Ω—É–∂–∂–Ω–æ", "–Ω—É–∂–Ω–Ω–æ", "–Ω—É–∂–µ–æ"],
            "–∫–æ–≥–¥–∞": ["–∫–∞–¥–∞", "–∫–∞–≥–¥–∞", "–∫–æ–≥–¥–¥–∞"],
            "—Å–∫–æ–ª—å–∫–æ": ["—Å–∫–æ–ª—å—å–∫–æ", "—Å–∫–æ–ª–∫–∞", "—Å–∫–æ–ª–∫–æ"]
        }
        
        # –°–ª–µ–Ω–≥ –∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è —Ä–µ—á—å
        self.slang_corrections = {
            "–Ω–æ—Ä–º": "–Ω–æ—Ä–º–∞–ª—å–Ω–æ",
            "–∫—É–ª": "–∫—Ä—É—Ç–æ", 
            "—Å—É–ø–µ—Ä": "–æ—Ç–ª–∏—á–Ω–æ",
            "–æ–∫": "—Ö–æ—Ä–æ—à–æ",
            "–æ–∫–µ–π": "—Ö–æ—Ä–æ—à–æ",
            "—Å–ø—Å": "—Å–ø–∞—Å–∏–±–æ",
            "–ø–∂–ª—Å—Ç": "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–ø–ª–∑": "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–∏–Ω—Ñ–∞": "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–∏–Ω—Ñ–æ": "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–ø—Ä–æ—Ñ": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π",
            "–º–∞–∫—Å": "–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π",
            "–º–∏–Ω": "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π",
            "–∫–æ–º–ø": "–∫–æ–º–ø—å—é—Ç–µ—Ä",
            "—á–µ–ª": "—á–µ–ª–æ–≤–µ–∫",
            "—á–µ–ª—ã": "–ª—é–¥–∏",
            "–º–±": "–º–æ–∂–µ—Ç –±—ã—Ç—å",
            "—Ö–∑": "–Ω–µ –∑–Ω–∞—é",
            "–ª–æ–ª": "",  # —É–¥–∞–ª—è–µ–º
            "–∫–µ–∫": "",  # —É–¥–∞–ª—è–µ–º
            "—Ç–æ–ø": "–æ—Ç–ª–∏—á–Ω—ã–π",
            "—Ñ–∏–≥–Ω—è": "–Ω–µ–≤–∞–∂–Ω–æ",
            "–∫—Ä—É—Ç—è–∫": "–æ—Ç–ª–∏—á–Ω–æ"
        }
        
        # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∞—Ç—å
        self.abbreviation_expansions = {
            "–¥—Ä.": "–¥—Ä—É–≥–∏–µ",
            "—Ç.–¥.": "—Ç–∞–∫ –¥–∞–ª–µ–µ", 
            "—Ç.–ø.": "—Ç–æ–º—É –ø–æ–¥–æ–±–Ω–æ–µ",
            "–∏ —Ç.–¥.": "–∏ —Ç–∞–∫ –¥–∞–ª–µ–µ",
            "–∏ —Ç.–ø.": "–∏ —Ç–æ–º—É –ø–æ–¥–æ–±–Ω–æ–µ",
            "—Å–º.": "—Å–º–æ—Ç—Ä–∏—Ç–µ",
            "—Å—Ç—Ä.": "—Å—Ç—Ä–∞–Ω–∏—Ü–∞",
            "–≥–ª.": "–≥–ª–∞–≤–∞",
            "—Ä–∞–∑–¥.": "—Ä–∞–∑–¥–µ–ª",
            "–ø.": "–ø—É–Ω–∫—Ç",
            "–ø–ø.": "–ø—É–Ω–∫—Ç—ã",
            "–Ω-—Ä": "–Ω–∞–ø—Ä–∏–º–µ—Ä",
            "–Ω–∞–ø—Ä.": "–Ω–∞–ø—Ä–∏–º–µ—Ä",
            "–º–≥": "–º–∏–ª–ª–∏–≥—Ä–∞–º–º",
            "–≥": "–≥—Ä–∞–º–º", 
            "–∫–≥": "–∫–∏–ª–æ–≥—Ä–∞–º–º",
            "–º–ª": "–º–∏–ª–ª–∏–ª–∏—Ç—Ä",
            "–ª": "–ª–∏—Ç—Ä",
            "—à—Ç": "—à—Ç—É–∫",
            "—É–ø.": "—É–ø–∞–∫–æ–≤–∫–∞",
            "—á.–ª.": "—á–∞–π–Ω–∞—è –ª–æ–∂–∫–∞",
            "—Å—Ç.–ª.": "—Å—Ç–æ–ª–æ–≤–∞—è –ª–æ–∂–∫–∞",
            "–∫–∫–∞–ª": "–∫–∏–ª–æ–∫–∞–ª–æ—Ä–∏–∏"
        }
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        self.cleaning_patterns = [
            # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            (r'\s+', ' '),
            # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            (r'\s+([,.!?;:])', r'\1'),
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            (r'([,.!?;:])(\w)', r'\1 \2'),
            # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            (r'[.]{2,}', '.'),
            (r'[!]{2,}', '!'),
            (r'[?]{2,}', '?'),
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–∫—Ä–æ–º–µ –Ω—É–∂–Ω—ã—Ö)
            (r'[^\w\s\-.,!?;:()\[\]/%¬∞]', ''),
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–µ—Ñ–∏—Å–æ–≤
            (r'[-‚àí‚Äì‚Äî]+', '-'),
            # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∫–æ–±–æ–∫
            (r'\(\s*\)', ''),
            (r'\[\s*\]', ''),
        ]
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–∞–ª–æ–∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑
        self.stop_phrases = {
            "–∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ", "–≤—Å–µ–º –∏–∑–≤–µ—Å—Ç–Ω–æ", "–æ—á–µ–≤–∏–¥–Ω–æ", "–ø–æ–Ω—è—Ç–Ω–æ", 
            "–∏—Ç–∞–∫", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ", "–≤ –æ–±—â–µ–º",
            "–≤–æ–æ–±—â–µ –≥–æ–≤–æ—Ä—è", "–∫—Å—Ç–∞—Ç–∏", "–º–µ–∂–¥—É –ø—Ä–æ—á–∏–º", "–Ω–∞–ø—Ä–∏–º–µ—Ä",
            "—Å–∫–∞–∂–µ–º", "–¥–æ–ø—É—Å—Ç–∏–º", "–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º", "–≤–æ–∑–º–æ–∂–Ω–æ"
        }
        
    def clean_text(self, text: str, source_info: str = "") -> str:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
            source_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text or not text.strip():
            return ""
        
        original_length = len(text)
        
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode
        text = self._normalize_unicode(text)
        
        # 2. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        text = self._fix_case_issues(text)
        
        # 3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫
        text = self._fix_typos(text)
        
        # 4. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–µ–Ω–≥–∞
        text = self._fix_slang(text)
        
        # 5. –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        text = self._expand_abbreviations(text)
        
        # 6. –û—á–∏—Å—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        text = self._clean_formatting(text)
        
        # 7. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑
        text = self._remove_stop_phrases(text)
        
        # 8. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = self._final_cleanup(text)
        
        cleaned_length = len(text)
        reduction_percent = ((original_length - cleaned_length) / original_length * 100) if original_length > 0 else 0
        
        if reduction_percent > 5:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ({source_info}): {original_length} ‚Üí {cleaned_length} —Å–∏–º–≤–æ–ª–æ–≤ (-{reduction_percent:.1f}%)")
        
        return text
    
    def _normalize_unicode(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode —Å–∏–º–≤–æ–ª–æ–≤"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ NFC —Ñ–æ—Ä–º–µ
        text = unicodedata.normalize('NFC', text)
        
        # –ó–∞–º–µ–Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        replacements = {
            '—ë': '–µ',  # –ó–∞–º–µ–Ω–∞ —ë –Ω–∞ –µ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
            '‚Ññ': '–Ω–æ–º–µ—Ä',
            '¬ß': '–ø–∞—Ä–∞–≥—Ä–∞—Ñ',
            '¬©': '',
            '¬Æ': '',
            '‚Ñ¢': '',
            # –†–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –∫–∞–≤—ã—á–µ–∫
            '"': '"', '"': '"', '‚Äû': '"', '‚Äö': "'", ''': "'", ''': "'",
            # –†–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –¥–µ—Ñ–∏—Å–æ–≤
            '‚Äì': '-', '‚Äî': '-', '‚àí': '-',
            # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            '√ó': 'x', '√∑': '/', '¬±': '+/-',
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text
    
    def _fix_case_issues(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º"""
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
        sentences = re.split(r'([.!?]\s+)', text)
        
        for i in range(0, len(sentences), 2):  # –ö–∞–∂–¥–æ–µ –≤—Ç–æ—Ä–æ–µ - —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            if sentences[i]:
                # –ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                sentences[i] = sentences[i][0].upper() + sentences[i][1:] if len(sentences[i]) > 1 else sentences[i].upper()
        
        text = ''.join(sentences)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        abbreviations = ['PDF', 'HTML', 'URL', 'API', 'FAQ', 'VIP', 'CEO', 'IT']
        for abbr in abbreviations:
            text = re.sub(f'\\b{abbr.lower()}\\b', abbr, text, flags=re.IGNORECASE)
        
        return text
    
    def _fix_typos(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫"""
        text_lower = text.lower()
        
        for correct, typos in self.typo_corrections.items():
            for typo in typos:
                # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –∑–∞–º–µ–Ω–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
                pattern = re.compile(re.escape(typo), re.IGNORECASE)
                text = pattern.sub(correct, text)
        
        return text
    
    def _fix_slang(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–µ–Ω–≥–∞ –∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–π —Ä–µ—á–∏"""
        words = text.split()
        
        for i, word in enumerate(words):
            word_clean = re.sub(r'[^\w]', '', word.lower())
            if word_clean in self.slang_corrections:
                replacement = self.slang_corrections[word_clean]
                if replacement:  # –ï—Å–ª–∏ –Ω–µ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                    words[i] = replacement
                else:  # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
                    words[i] = ""
        
        return ' '.join(filter(None, words))
    
    def _expand_abbreviations(self, text: str) -> str:
        """–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π"""
        for abbr, expansion in self.abbreviation_expansions.items():
            # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –∑–∞–º–µ–Ω–∞
            pattern = r'\b' + re.escape(abbr) + r'\b'
            text = re.sub(pattern, expansion, text, flags=re.IGNORECASE)
        
        return text
    
    def _clean_formatting(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        for pattern, replacement in self.cleaning_patterns:
            text = re.sub(pattern, replacement, text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not re.match(r'^[\d\s\-.,()]+$', line):  # –ù–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã –∏ –∑–Ω–∞–∫–∏
                if len(line) > 3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏
                    cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _remove_stop_phrases(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –º–∞–ª–æ–∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑"""
        for phrase in self.stop_phrases:
            # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—ã –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            pattern = r'\b' + re.escape(phrase) + r'[,\s]*'
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text
    
    def _final_cleanup(self, text: str) -> str:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        text = '\n'.join(lines)
        
        return text
    
    def remove_duplicates(self, texts: List[str]) -> List[str]:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not texts:
            return []
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        seen = set()
        unique_texts = []
        duplicates_count = 0
        
        for text in texts:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
            normalized = re.sub(r'\s+', ' ', text.strip().lower())
            
            if normalized and normalized not in seen:
                seen.add(normalized)
                unique_texts.append(text)
            else:
                duplicates_count += 1
        
        if duplicates_count > 0:
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤")
        
        return unique_texts
    
    def filter_quality_texts(self, texts: List[str], min_length: int = 50) -> List[str]:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        """
        quality_texts = []
        filtered_count = 0
        
        for text in texts:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
            if len(text.strip()) < min_length:
                filtered_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –±—É–∫–≤ –∫ —Å–∏–º–≤–æ–ª–∞–º
            letters = len(re.findall(r'[–∞-—è—ë]', text.lower()))
            total_chars = len(text)
            letter_ratio = letters / total_chars if total_chars > 0 else 0
            
            if letter_ratio < 0.6:  # –ú–∏–Ω–∏–º—É–º 60% –±—É–∫–≤
                filtered_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            service_patterns = [
                r'^\d+$',  # –¢–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
                r'^—Å—Ç—Ä\.\s*\d+',  # –ù–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
                r'^–≥–ª–∞–≤–∞\s*\d+',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≥–ª–∞–≤
                r'^—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ',  # –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ
                r'^\.\.\.',  # –ú–Ω–æ–≥–æ—Ç–æ—á–∏—è
            ]
            
            is_service = any(re.match(pattern, text.strip().lower()) for pattern in service_patterns)
            if is_service:
                filtered_count += 1
                continue
            
            quality_texts.append(text)
        
        if filtered_count > 0:
            logger.info(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
        
        return quality_texts
    
    def clean_document_batch(self, documents: List[Dict]) -> List[Dict]:
        """
        –û—á–∏—Å—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ 'text' –∏ 'metadata'
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not documents:
            return []
        
        logger.info(f"üßπ –ù–∞—á–∏–Ω–∞—é –æ—á–∏—Å—Ç–∫—É {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        cleaned_documents = []
        total_original_length = 0
        total_cleaned_length = 0
        
        for doc in documents:
            if 'text' not in doc:
                continue
            
            original_text = doc['text']
            original_length = len(original_text)
            total_original_length += original_length
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            source_info = f"{doc.get('metadata', {}).get('source', 'Unknown')}"
            cleaned_text = self.clean_text(original_text, source_info)
            
            if cleaned_text.strip():  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –∑–Ω–∞—á–∏–º—ã–π —Ç–µ–∫—Å—Ç
                doc_copy = doc.copy()
                doc_copy['text'] = cleaned_text
                cleaned_documents.append(doc_copy)
                total_cleaned_length += len(cleaned_text)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        texts_for_dedup = [doc['text'] for doc in cleaned_documents]
        unique_texts = self.remove_duplicates(texts_for_dedup)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        final_documents = []
        for i, unique_text in enumerate(unique_texts):
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
            for doc in cleaned_documents:
                if doc['text'] == unique_text:
                    final_documents.append(doc)
                    break
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        quality_texts = self.filter_quality_texts([doc['text'] for doc in final_documents])
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
        result_documents = []
        for quality_text in quality_texts:
            for doc in final_documents:
                if doc['text'] == quality_text:
                    result_documents.append(doc)
                    break
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        reduction_percent = ((total_original_length - total_cleaned_length) / total_original_length * 100) if total_original_length > 0 else 0
        
        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)} ‚Üí {len(result_documents)}")
        logger.info(f"   üìù –û–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {total_original_length} ‚Üí {total_cleaned_length} —Å–∏–º–≤–æ–ª–æ–≤ (-{reduction_percent:.1f}%)")
        logger.info(f"   üéØ –ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: —É–¥–∞–ª–µ–Ω—ã –æ–ø–µ—á–∞—Ç–∫–∏, —Å–ª–µ–Ω–≥, –¥—É–±–ª–∏–∫–∞—Ç—ã")
        
        return result_documents
    
    def get_cleaning_stats(self, original_text: str, cleaned_text: str) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—á–∏—Å—Ç–∫–∏
        
        Args:
            original_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            cleaned_text: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return {
            "original_length": len(original_text),
            "cleaned_length": len(cleaned_text),
            "reduction_percent": ((len(original_text) - len(cleaned_text)) / len(original_text) * 100) if len(original_text) > 0 else 0,
            "original_words": len(original_text.split()),
            "cleaned_words": len(cleaned_text.split()),
            "typos_fixed": sum(1 for typos in self.typo_corrections.values() for typo in typos if typo in original_text.lower()),
            "slang_fixed": sum(1 for slang in self.slang_corrections.keys() if slang in original_text.lower())
        }
